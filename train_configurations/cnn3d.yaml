# Optimization
tune_config:
      #intermediate_dim: tune.choice([20, 40, 50])
      epochs: tune.choice([100, 200, 300])
      lr: tune.choice([0.003, 0.001, 0.0009, 0.0003, 0.0001])
      lr_patience: tune.choice([3, 5, 7, 10])
      act: tune.choice(['Relu', 'Elu', 'Selu', 'LRelu'])
      batch_size: tune.choice([10, 20, 40])
      patch_size: tune.choice([7, 9, 11, 15, 25])
      num_filter: tune.choice([10, 20, 40])
      filter_size: tune.choice([3, 5])
      #l1: 'tune.sample_from(lambda _: 2 ** np.random.randint(2, 9))'
      #l2: 'tune.sample_from(lambda _: 2 ** np.random.randint(2, 9))'
      #lr: 'tune.loguniform(1e-4, 1e-1)'
  
# Dataset
dataset:
  name: 'albania_supervised'
  sequential: False
  in_channel: 24
  #patch_size: 7
  coords_path: "/home/roberto/Documents/backup_rob/esa/fdir/data/hyper/albania/mosaic/selected_pixel_32.pkl" #Take relative path
  test_coords_path: "/home/roberto/Documents/backup_rob/esa/fdir/data/hyper/albania/mosaic/test/test.pkl"
  train_split: 0.70

# Model
model:
  name: 'cnn3d'
  dilation: 1
  class_number: 1 # switchig to crossentropu use 2 n_ckasse that it means 2 output neurons >>> crossenetropy loss (not binary) with Softmax out of the box
  n_channel: 24

resources:
  gpu_trial: 0.25
  cpu_trial: 4

opt:
  fine_tuning: False
  metrics: ['val_loss', 'val_acc', 'val_f1', 'test_acc', 'test_f1']
  order_by: 'val_f1'


