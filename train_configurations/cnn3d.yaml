# Optimization
tune_config:
      #intermediate_dim: tune.choice([20, 40, 50])
      epochs: tune.choice([200])
      lr: tune.choice([0.0001])
      lr_patience: tune.choice([5])
      act: tune.choice(['LRelu'])
      batch_size: tune.choice([20])
      patch_size: tune.choice([7])
      num_filter: tune.choice([40])
      filter_size: tune.choice([3])
      augmentation: tune.choice([0])
      oversampling: tune.choice([0])
      optimizer_name: tune.choice(['adam'])

      #l1: 'tune.sample_from(lambda _: 2 ** np.random.randint(2, 9))'
      #l2: 'tune.sample_from(lambda _: 2 ** np.random.randint(2, 9))'
      #lr: 'tune.loguniform(1e-4, 1e-1)'

# Dataset
dataset:
  name: 'albania_supervised'
  sequential: False
  in_channel: 288
  coords_path: "/data/hyper/albania/mosaic/train_val_dict_merged_12k.pkl" #Take relative path
  test_coords_path: "/data/hyper/albania/mosaic/train_val_dict_merged_12k.pkl"
  train_split: 0.70
  from_dictionary: True

# Model
model:
  name: 'cnn3d'
  dilation: 1
  class_number: 1 # switchig to crossentropu use 2 n_ckasse that it means 2 output neurons >>> crossenetropy loss (not binary) with Softmax out of the box

resources:
  gpu_trial: 1
  cpu_trial: 12

opt:
  fine_tuning: False
  tune_report: 'val_loss'
  k_fold_cv: 0
  metrics: ['train_loss', 'val_loss', 'val_acc', 'val_f1', 'test_acc', 'test_f1']
  order_by: 'val_f1'
  num_workers: 0
