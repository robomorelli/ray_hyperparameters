# Optimization
tune_config:
      intermediate_dim: tune.choice([1, 2, 5, 10, 20, 40, 60, 80, 100])
      epochs: tune.choice([100, 120, 140, 180, 200])
      lr: tune.choice([0.0001, 0.0003, 0.0006, 0.0009, 0.001, 0.003])
      batch_size: tune.choice([20, 60, 100, 200, 400, 600, 1000])
      seq_in_length: tune.choice([3,5,7,10, 15])
      embedding_dim: tune.choice([2, 4, 8,16,32,64, 128])
      latent_dim: tune.choice([1,2,4,10,20,40,80])
      n_layers: tune.choice([1,2,3,4])
      lr_patience: tune.choice([3,5,7])

# Dataset
dataset:
  name: 'sentinel'
  sequential: False
  sample_rate: "4s"
  scaled: True
  clean: True
  shuffle: False #shuffle applied on the row of dataset
  feats: 'all'
  columns: ['RW1_motcurr', 'RW2_motcurr', 'RW3_motcurr', 'RW4_motcurr', 'RW1_therm',
          'RW2_therm', 'RW3_therm', 'RW4_therm', 'RW1_speed', 'RW2_speed',
          'RW3_speed', 'RW4_speed', 'RW1_cmd_volt', 'RW2_cmd_volt',
          'RW3_cmd_volt', 'RW4_cmd_volt']
  dataset_subset: 100000
  columns_subset: 0
  target: None
  predict: False
  train_val_split: 0.8

# Model
model:
  name: 'lstm_ae'

resources:
  gpu_trial: 1
  cpu_trial: 12

opt:
  fine_tuning: False
  tune_report: 'val_loss'
  k_fold_cv: 0
  metrics: ['train_loss', 'val_loss']
  order_by: 'val_loss'
  num_workers: 0

